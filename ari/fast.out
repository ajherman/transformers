[2024-06-24 11:48:46,414] torch.distributed.run: [WARNING] 
[2024-06-24 11:48:46,414] torch.distributed.run: [WARNING] *****************************************
[2024-06-24 11:48:46,414] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-24 11:48:46,414] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
Finished loading datasets
Finished loading datasets
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1651.83 examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1648.58 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1631.08 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1640.67 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1675.98 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1691.55 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1713.16 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1654.26 examples/s]
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
Map: 100%|██████████| 3760/3760 [00:02<00:00, 1729.48 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1658.60 examples/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
Finished loading datasets
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
cn1:1631493:1631493 [0] NCCL INFO Bootstrap : Using ib0:192.168.81.2<0>
cn1:1631493:1631493 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn1:1631493:1631493 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.19.3+cuda12.3
cn1:1631494:1631494 [1] NCCL INFO cudaDriverVersion 12040
cn1:1631494:1631494 [1] NCCL INFO Bootstrap : Using ib0:192.168.81.2<0>
cn1:1631494:1631494 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn1:1631493:1631635 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.2<0>
cn1:1631493:1631635 [0] NCCL INFO Using non-device net plugin version 0
cn1:1631493:1631635 [0] NCCL INFO Using network IB
cn1:1631494:1631636 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.2<0>
cn1:1631494:1631636 [1] NCCL INFO Using non-device net plugin version 0
cn1:1631494:1631636 [1] NCCL INFO Using network IB
Finished loading datasets
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1659.81 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1653.07 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1703.71 examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1739.23 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1635.18 examples/s]
Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1646.35 examples/s]max_steps is given, it will override any value given in num_train_epochs
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
cn1:1631495:1631495 [2] NCCL INFO cudaDriverVersion 12040
cn1:1631495:1631495 [2] NCCL INFO Bootstrap : Using ib0:192.168.81.2<0>
cn1:1631495:1631495 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn1:1631495:1631649 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.2<0>
cn1:1631495:1631649 [2] NCCL INFO Using non-device net plugin version 0
cn1:1631495:1631649 [2] NCCL INFO Using network IB
Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1633.74 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1683.38 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1723.39 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1662.21 examples/s]
max_steps is given, it will override any value given in num_train_epochs
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
cn1:1631496:1631496 [3] NCCL INFO cudaDriverVersion 12040
cn1:1631496:1631496 [3] NCCL INFO Bootstrap : Using ib0:192.168.81.2<0>
cn1:1631496:1631496 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn1:1631496:1631657 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.2<0>
cn1:1631496:1631657 [3] NCCL INFO Using non-device net plugin version 0
cn1:1631496:1631657 [3] NCCL INFO Using network IB
cn1:1631496:1631657 [3] NCCL INFO comm 0x9ffd5c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0xb53d8aea2a6edc57 - Init START
cn1:1631495:1631649 [2] NCCL INFO comm 0xb232140 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xb53d8aea2a6edc57 - Init START
cn1:1631494:1631636 [1] NCCL INFO comm 0xa9d53c0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 41000 commId 0xb53d8aea2a6edc57 - Init START
cn1:1631493:1631635 [0] NCCL INFO comm 0xa89edc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0xb53d8aea2a6edc57 - Init START
cn1:1631496:1631657 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff,00000000,00000000
cn1:1631496:1631657 [3] NCCL INFO NVLS multicast support is not available on dev 3
cn1:1631494:1631636 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff
cn1:1631494:1631636 [1] NCCL INFO NVLS multicast support is not available on dev 1
cn1:1631493:1631635 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff
cn1:1631493:1631635 [0] NCCL INFO NVLS multicast support is not available on dev 0
cn1:1631495:1631649 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff,00000000,00000000
cn1:1631495:1631649 [2] NCCL INFO NVLS multicast support is not available on dev 2
cn1:1631493:1631635 [0] NCCL INFO Channel 00/24 :    0   1   2   3
cn1:1631493:1631635 [0] NCCL INFO Channel 01/24 :    0   1   3   2
cn1:1631493:1631635 [0] NCCL INFO Channel 02/24 :    0   2   3   1
cn1:1631493:1631635 [0] NCCL INFO Channel 03/24 :    0   2   1   3
cn1:1631493:1631635 [0] NCCL INFO Channel 04/24 :    0   3   1   2
cn1:1631493:1631635 [0] NCCL INFO Channel 05/24 :    0   3   2   1
cn1:1631495:1631649 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
cn1:1631493:1631635 [0] NCCL INFO Channel 06/24 :    0   1   2   3
cn1:1631496:1631657 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
cn1:1631493:1631635 [0] NCCL INFO Channel 07/24 :    0   1   3   2
cn1:1631495:1631649 [2] NCCL INFO P2P Chunksize set to 524288
cn1:1631494:1631636 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
cn1:1631493:1631635 [0] NCCL INFO Channel 08/24 :    0   2   3   1
cn1:1631496:1631657 [3] NCCL INFO P2P Chunksize set to 524288
cn1:1631493:1631635 [0] NCCL INFO Channel 09/24 :    0   2   1   3
cn1:1631494:1631636 [1] NCCL INFO P2P Chunksize set to 524288
cn1:1631493:1631635 [0] NCCL INFO Channel 10/24 :    0   3   1   2
cn1:1631493:1631635 [0] NCCL INFO Channel 11/24 :    0   3   2   1
cn1:1631493:1631635 [0] NCCL INFO Channel 12/24 :    0   1   2   3
cn1:1631493:1631635 [0] NCCL INFO Channel 13/24 :    0   1   3   2
cn1:1631493:1631635 [0] NCCL INFO Channel 14/24 :    0   2   3   1
cn1:1631493:1631635 [0] NCCL INFO Channel 15/24 :    0   2   1   3
cn1:1631493:1631635 [0] NCCL INFO Channel 16/24 :    0   3   1   2
cn1:1631493:1631635 [0] NCCL INFO Channel 17/24 :    0   3   2   1
cn1:1631493:1631635 [0] NCCL INFO Channel 18/24 :    0   1   2   3
cn1:1631493:1631635 [0] NCCL INFO Channel 19/24 :    0   1   3   2
cn1:1631493:1631635 [0] NCCL INFO Channel 20/24 :    0   2   3   1
cn1:1631493:1631635 [0] NCCL INFO Channel 21/24 :    0   2   1   3
cn1:1631493:1631635 [0] NCCL INFO Channel 22/24 :    0   3   1   2
cn1:1631493:1631635 [0] NCCL INFO Channel 23/24 :    0   3   2   1
cn1:1631493:1631635 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
cn1:1631493:1631635 [0] NCCL INFO P2P Chunksize set to 524288
cn1:1631496:1631657 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Connected all rings
cn1:1631495:1631649 [2] NCCL INFO Connected all rings
cn1:1631494:1631636 [1] NCCL INFO Connected all rings
cn1:1631493:1631635 [0] NCCL INFO Connected all rings
cn1:1631493:1631635 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631496:1631657 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631494:1631636 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631495:1631649 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn1:1631493:1631635 [0] NCCL INFO Connected all trees
cn1:1631495:1631649 [2] NCCL INFO Connected all trees
cn1:1631496:1631657 [3] NCCL INFO Connected all trees
cn1:1631495:1631649 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn1:1631495:1631649 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn1:1631494:1631636 [1] NCCL INFO Connected all trees
cn1:1631496:1631657 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn1:1631496:1631657 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn1:1631494:1631636 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn1:1631494:1631636 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn1:1631493:1631635 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn1:1631493:1631635 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn1:1631495:1631649 [2] NCCL INFO comm 0xb232140 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xb53d8aea2a6edc57 - Init COMPLETE
cn1:1631493:1631635 [0] NCCL INFO comm 0xa89edc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0xb53d8aea2a6edc57 - Init COMPLETE
cn1:1631496:1631657 [3] NCCL INFO comm 0x9ffd5c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0xb53d8aea2a6edc57 - Init COMPLETE
cn1:1631494:1631636 [1] NCCL INFO comm 0xa9d53c0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 41000 commId 0xb53d8aea2a6edc57 - Init COMPLETE
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	per_device_train_batch_size: 13 (from args) != 16 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	per_device_train_batch_size: 13 (from args) != 16 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	per_device_train_batch_size: 13 (from args) != 16 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	per_device_train_batch_size: 13 (from args) != 16 (from trainer_state.json)
