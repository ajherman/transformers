[2024-06-13 21:59:46,101] torch.distributed.run: [WARNING] 
[2024-06-13 21:59:46,101] torch.distributed.run: [WARNING] *****************************************
[2024-06-13 21:59:46,101] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-13 21:59:46,101] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
The JSON loader parameter `block_size` is deprecated. Please use `chunksize` instead
Finished loading datasets
Finished loading datasets
Finished loading datasets
Finished loading datasets
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1565.63 examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1587.43 examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1554.36 examples/s]Map:  27%|██▋       | 1000/3760 [00:00<00:01, 1598.39 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1556.47 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1585.63 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1555.00 examples/s]Map:  53%|█████▎    | 2000/3760 [00:01<00:01, 1569.36 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1605.70 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1634.06 examples/s]Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1603.76 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1647.39 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1588.66 examples/s]
Map:  80%|███████▉  | 3000/3760 [00:01<00:00, 1615.37 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1675.62 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1620.04 examples/s]
Map: 100%|██████████| 3760/3760 [00:02<00:00, 1644.58 examples/s]Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Map: 100%|██████████| 3760/3760 [00:02<00:00, 1584.53 examples/s]
max_steps is given, it will override any value given in num_train_epochs
Exception occurred: No valid checkpoint found in output directory (original)
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 208, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1853, in train
    raise ValueError(f"No valid checkpoint found in output directory ({args.output_dir})")
ValueError: No valid checkpoint found in output directory (original)
Map: 100%|██████████| 3760/3760 [00:02<00:00, 1654.36 examples/s]Map: 100%|██████████| 3760/3760 [00:02<00:00, 1601.95 examples/s]
max_steps is given, it will override any value given in num_train_epochs
Exception occurred: No valid checkpoint found in output directory (original)
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 208, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1853, in train
    raise ValueError(f"No valid checkpoint found in output directory ({args.output_dir})")
ValueError: No valid checkpoint found in output directory (original)
max_steps is given, it will override any value given in num_train_epochs
Exception occurred: No valid checkpoint found in output directory (original)
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 208, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1853, in train
    raise ValueError(f"No valid checkpoint found in output directory ({args.output_dir})")
ValueError: No valid checkpoint found in output directory (original)
max_steps is given, it will override any value given in num_train_epochs
Exception occurred: No valid checkpoint found in output directory (original)
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 208, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1853, in train
    raise ValueError(f"No valid checkpoint found in output directory ({args.output_dir})")
ValueError: No valid checkpoint found in output directory (original)
