[2024-06-17 17:54:05,716] torch.distributed.run: [WARNING] 
[2024-06-17 17:54:05,716] torch.distributed.run: [WARNING] *****************************************
[2024-06-17 17:54:05,716] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-17 17:54:05,716] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
cn0:84774:84774 [0] NCCL INFO Bootstrap : Using ib0:192.168.81.1<0>
cn0:84774:84774 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn0:84774:84774 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.19.3+cuda12.3
cn0:84775:84775 [1] NCCL INFO cudaDriverVersion 12040
cn0:84776:84776 [2] NCCL INFO cudaDriverVersion 12040
cn0:84775:84775 [1] NCCL INFO Bootstrap : Using ib0:192.168.81.1<0>
cn0:84776:84776 [2] NCCL INFO Bootstrap : Using ib0:192.168.81.1<0>
cn0:84776:84776 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn0:84775:84775 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn0:84777:84777 [3] NCCL INFO cudaDriverVersion 12040
cn0:84777:84777 [3] NCCL INFO Bootstrap : Using ib0:192.168.81.1<0>
cn0:84777:84777 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn0:84776:84874 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.1<0>
cn0:84776:84874 [2] NCCL INFO Using non-device net plugin version 0
cn0:84776:84874 [2] NCCL INFO Using network IB
cn0:84775:84873 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.1<0>
cn0:84777:84875 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.1<0>
cn0:84775:84873 [1] NCCL INFO Using non-device net plugin version 0
cn0:84775:84873 [1] NCCL INFO Using network IB
cn0:84777:84875 [3] NCCL INFO Using non-device net plugin version 0
cn0:84777:84875 [3] NCCL INFO Using network IB
cn0:84774:84872 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:192.168.81.1<0>
cn0:84774:84872 [0] NCCL INFO Using non-device net plugin version 0
cn0:84774:84872 [0] NCCL INFO Using network IB
cn0:84777:84875 [3] NCCL INFO comm 0x204b6630 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0xa10db80f6729ad9d - Init START
cn0:84776:84874 [2] NCCL INFO comm 0x1e721250 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xa10db80f6729ad9d - Init START
cn0:84775:84873 [1] NCCL INFO comm 0x1f7ee2b0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 41000 commId 0xa10db80f6729ad9d - Init START
cn0:84774:84872 [0] NCCL INFO comm 0x1eb23dd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0xa10db80f6729ad9d - Init START
cn0:84777:84875 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff,00000000,00000000
cn0:84777:84875 [3] NCCL INFO NVLS multicast support is not available on dev 3
cn0:84775:84873 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff
cn0:84775:84873 [1] NCCL INFO NVLS multicast support is not available on dev 1
cn0:84776:84874 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff,00000000,00000000
cn0:84776:84874 [2] NCCL INFO NVLS multicast support is not available on dev 2
cn0:84774:84872 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffffffff,00000000,00000000,ffffffff,ffffffff
cn0:84774:84872 [0] NCCL INFO NVLS multicast support is not available on dev 0
cn0:84774:84872 [0] NCCL INFO Channel 00/24 :    0   1   2   3
cn0:84774:84872 [0] NCCL INFO Channel 01/24 :    0   1   3   2
cn0:84774:84872 [0] NCCL INFO Channel 02/24 :    0   2   3   1
cn0:84774:84872 [0] NCCL INFO Channel 03/24 :    0   2   1   3
cn0:84774:84872 [0] NCCL INFO Channel 04/24 :    0   3   1   2
cn0:84774:84872 [0] NCCL INFO Channel 05/24 :    0   3   2   1
cn0:84777:84875 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
cn0:84774:84872 [0] NCCL INFO Channel 06/24 :    0   1   2   3
cn0:84774:84872 [0] NCCL INFO Channel 07/24 :    0   1   3   2
cn0:84777:84875 [3] NCCL INFO P2P Chunksize set to 524288
cn0:84774:84872 [0] NCCL INFO Channel 08/24 :    0   2   3   1
cn0:84774:84872 [0] NCCL INFO Channel 09/24 :    0   2   1   3
cn0:84774:84872 [0] NCCL INFO Channel 10/24 :    0   3   1   2
cn0:84776:84874 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
cn0:84774:84872 [0] NCCL INFO Channel 11/24 :    0   3   2   1
cn0:84774:84872 [0] NCCL INFO Channel 12/24 :    0   1   2   3
cn0:84776:84874 [2] NCCL INFO P2P Chunksize set to 524288
cn0:84775:84873 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
cn0:84774:84872 [0] NCCL INFO Channel 13/24 :    0   1   3   2
cn0:84774:84872 [0] NCCL INFO Channel 14/24 :    0   2   3   1
cn0:84775:84873 [1] NCCL INFO P2P Chunksize set to 524288
cn0:84774:84872 [0] NCCL INFO Channel 15/24 :    0   2   1   3
cn0:84774:84872 [0] NCCL INFO Channel 16/24 :    0   3   1   2
cn0:84774:84872 [0] NCCL INFO Channel 17/24 :    0   3   2   1
cn0:84774:84872 [0] NCCL INFO Channel 18/24 :    0   1   2   3
cn0:84774:84872 [0] NCCL INFO Channel 19/24 :    0   1   3   2
cn0:84774:84872 [0] NCCL INFO Channel 20/24 :    0   2   3   1
cn0:84774:84872 [0] NCCL INFO Channel 21/24 :    0   2   1   3
cn0:84774:84872 [0] NCCL INFO Channel 22/24 :    0   3   1   2
cn0:84774:84872 [0] NCCL INFO Channel 23/24 :    0   3   2   1
cn0:84774:84872 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
cn0:84774:84872 [0] NCCL INFO P2P Chunksize set to 524288
cn0:84775:84873 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Connected all rings
cn0:84774:84872 [0] NCCL INFO Connected all rings
cn0:84776:84874 [2] NCCL INFO Connected all rings
cn0:84775:84873 [1] NCCL INFO Connected all rings
cn0:84774:84872 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84774:84872 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84777:84875 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84775:84873 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
cn0:84776:84874 [2] NCCL INFO Connected all trees
cn0:84775:84873 [1] NCCL INFO Connected all trees
cn0:84774:84872 [0] NCCL INFO Connected all trees
cn0:84776:84874 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn0:84776:84874 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn0:84777:84875 [3] NCCL INFO Connected all trees
cn0:84777:84875 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn0:84777:84875 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn0:84774:84872 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn0:84774:84872 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn0:84775:84873 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn0:84775:84873 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
cn0:84774:84872 [0] NCCL INFO comm 0x1eb23dd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0xa10db80f6729ad9d - Init COMPLETE
cn0:84776:84874 [2] NCCL INFO comm 0x1e721250 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xa10db80f6729ad9d - Init COMPLETE
cn0:84775:84873 [1] NCCL INFO comm 0x1f7ee2b0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 41000 commId 0xa10db80f6729ad9d - Init COMPLETE
cn0:84777:84875 [3] NCCL INFO comm 0x204b6630 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0xa10db80f6729ad9d - Init COMPLETE
  0%|          | 0/79 [00:00<?, ?it/s]  3%|▎         | 2/79 [00:00<00:07,  9.81it/s]  4%|▍         | 3/79 [00:00<00:10,  7.31it/s]  5%|▌         | 4/79 [00:01<00:33,  2.24it/s]Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/test.py", line 89, in <module>
    results = trainer.evaluate()
              ^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3572, in evaluate
    output = eval_loop(Traceback (most recent call last):

       File "/vast/home/ajherman/transformers/ari/test.py", line 89, in <module>
        ^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3779, in evaluation_loop
    results = trainer.evaluate()
              ^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3572, in evaluate
    all_preds.add(logits)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 326, in add
    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 140, in nested_concat
        output = eval_loop(return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)

             ^^^^ ^ ^ ^ ^^ ^ 
      File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3779, in evaluation_loop
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 99, in torch_pad_and_concatenate
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/test.py", line 89, in <module>
    return torch.cat((tensor1, tensor2), dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.01 GiB. GPU 2 has a total capacity of 39.38 GiB of which 14.17 GiB is free. Including non-PyTorch memory, this process has 25.20 GiB memory in use. Of the allocated memory 23.49 GiB is allocated by PyTorch, and 47.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    results = trainer.evaluate()
              ^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3572, in evaluate
    all_preds.add(logits)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 326, in add
    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 140, in nested_concat
    output = eval_loop(
             ^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3779, in evaluation_loop
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 99, in torch_pad_and_concatenate
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/test.py", line 89, in <module>
    return torch.cat((tensor1, tensor2), dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.01 GiB. GPU 3 has a total capacity of 39.38 GiB of which 14.24 GiB is free. Including non-PyTorch memory, this process has 25.13 GiB memory in use. Of the allocated memory 23.49 GiB is allocated by PyTorch, and 47.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    all_preds.add(logits)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 326, in add
    results = trainer.evaluate()
              ^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3572, in evaluate
    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 140, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 99, in torch_pad_and_concatenate
    output = eval_loop(
             ^^^^^^^^    ^return torch.cat((tensor1, tensor2), dim=0)^

  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer.py", line 3779, in evaluation_loop
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.01 GiB. GPU 1 has a total capacity of 39.38 GiB of which 14.17 GiB is free. Including non-PyTorch memory, this process has 25.20 GiB memory in use. Of the allocated memory 23.49 GiB is allocated by PyTorch, and 47.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    all_preds.add(logits)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 326, in add
    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 140, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 99, in torch_pad_and_concatenate
    return torch.cat((tensor1, tensor2), dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.01 GiB. GPU 0 has a total capacity of 39.38 GiB of which 14.24 GiB is free. Including non-PyTorch memory, this process has 25.13 GiB memory in use. Of the allocated memory 23.49 GiB is allocated by PyTorch, and 47.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
cn0:84774:84883 [0] NCCL INFO [Service thread] Connection closed by localRank 2
cn0:84776:84882 [2] NCCL INFO [Service thread] Connection closed by localRank 2
cn0:84777:84880 [3] NCCL INFO [Service thread] Connection closed by localRank 2
cn0:84775:84881 [1] NCCL INFO [Service thread] Connection closed by localRank 2
cn0:84774:84883 [0] NCCL INFO [Service thread] Connection closed by localRank 1
cn0:84776:84882 [2] NCCL INFO [Service thread] Connection closed by localRank 1
cn0:84777:84880 [3] NCCL INFO [Service thread] Connection closed by localRank 1
cn0:84775:84881 [1] NCCL INFO [Service thread] Connection closed by localRank 1
cn0:84774:84883 [0] NCCL INFO [Service thread] Connection closed by localRank 3
cn0:84777:84880 [3] NCCL INFO [Service thread] Connection closed by localRank 3
cn0:84776:84882 [2] NCCL INFO [Service thread] Connection closed by localRank 3
cn0:84775:84881 [1] NCCL INFO [Service thread] Connection closed by localRank 3
  5%|▌         | 4/79 [00:02<00:54,  1.38it/s]
cn0:84774:84883 [0] NCCL INFO [Service thread] Connection closed by localRank 0
cn0:84776:84882 [2] NCCL INFO [Service thread] Connection closed by localRank 0
cn0:84777:84880 [3] NCCL INFO [Service thread] Connection closed by localRank 0
cn0:84775:84881 [1] NCCL INFO [Service thread] Connection closed by localRank 0
cn0:84777:84777 [3] NCCL INFO comm 0x204b6630 rank 3 nranks 4 cudaDev 3 busId c1000 - Abort COMPLETE
cn0:84775:84775 [1] NCCL INFO comm 0x1f7ee2b0 rank 1 nranks 4 cudaDev 1 busId 41000 - Abort COMPLETE
cn0:84776:84776 [2] NCCL INFO comm 0x1e721250 rank 2 nranks 4 cudaDev 2 busId 81000 - Abort COMPLETE
cn0:84774:84774 [0] NCCL INFO comm 0x1eb23dd0 rank 0 nranks 4 cudaDev 0 busId 1000 - Abort COMPLETE
[2024-06-17 17:54:25,729] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 84774) of binary: /vast/home/ajherman/miniconda3/envs/transformer/bin/python
Traceback (most recent call last):
  File "/vast/home/ajherman/miniconda3/envs/transformer/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
test.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-17_17:54:25
  host      : cn0
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 84775)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-06-17_17:54:25
  host      : cn0
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 84776)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-06-17_17:54:25
  host      : cn0
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 84777)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-17_17:54:25
  host      : cn0
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 84774)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
