[2024-06-05 13:55:23,647] torch.distributed.run: [WARNING] 
[2024-06-05 13:55:23,647] torch.distributed.run: [WARNING] *****************************************
[2024-06-05 13:55:23,647] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-05 13:55:23,647] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Finished loading datasets
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Finished loading datasets
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Exception occurred: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 211, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 580, in __init__
    raise ValueError(
ValueError: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.
Exception occurred: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 211, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 580, in __init__
    raise ValueError(
ValueError: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.
