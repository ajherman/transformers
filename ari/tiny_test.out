[2024-06-07 09:51:06,359] torch.distributed.run: [WARNING] 
[2024-06-07 09:51:06,359] torch.distributed.run: [WARNING] *****************************************
[2024-06-07 09:51:06,359] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-07 09:51:06,359] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Finished loading datasets
/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Finished loading datasets
/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Finished loading datasets
/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
You are resuming training from a checkpoint trained with 4.41.0.dev0 of Transformers but your current version is 4.41.2. This is not recommended and could yield to errors or unwanted behaviors.
max_steps is given, it will override any value given in num_train_epochs
You are resuming training from a checkpoint trained with 4.41.0.dev0 of Transformers but your current version is 4.41.2. This is not recommended and could yield to errors or unwanted behaviors.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
You are resuming training from a checkpoint trained with 4.41.0.dev0 of Transformers but your current version is 4.41.2. This is not recommended and could yield to errors or unwanted behaviors.
Dataset info for monology/pile-uncopyrighted is not completely ready yet.
No config specified, defaulting to the single config: pile-uncopyrighted/default
Loading Dataset Infos from /vast/home/ajherman/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Finished loading datasets
/vast/home/ajherman/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
You are resuming training from a checkpoint trained with 4.41.0.dev0 of Transformers but your current version is 4.41.2. This is not recommended and could yield to errors or unwanted behaviors.
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
cn4070:3740706:3740706 [0] NCCL INFO Bootstrap : Using eth0:192.168.101.13<0>
cn4070:3740706:3740706 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn4070:3740706:3740706 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.19.3+cuda12.3
cn4070:3740709:3740709 [3] NCCL INFO cudaDriverVersion 12040
cn4070:3740709:3740709 [3] NCCL INFO Bootstrap : Using eth0:192.168.101.13<0>
cn4070:3740709:3740709 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn4070:3740707:3740707 [1] NCCL INFO cudaDriverVersion 12040
cn4070:3740707:3740707 [1] NCCL INFO Bootstrap : Using eth0:192.168.101.13<0>
cn4070:3740707:3740707 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn4070:3740708:3740708 [2] NCCL INFO cudaDriverVersion 12040
cn4070:3740708:3740708 [2] NCCL INFO Bootstrap : Using eth0:192.168.101.13<0>
cn4070:3740708:3740708 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn4070:3740709:3740871 [3] NCCL INFO NET/IB : No device found.
cn4070:3740709:3740871 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.101.13<0>
cn4070:3740709:3740871 [3] NCCL INFO Using non-device net plugin version 0
cn4070:3740709:3740871 [3] NCCL INFO Using network Socket
cn4070:3740706:3740870 [0] NCCL INFO NET/IB : No device found.
cn4070:3740706:3740870 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.101.13<0>
cn4070:3740706:3740870 [0] NCCL INFO Using non-device net plugin version 0
cn4070:3740706:3740870 [0] NCCL INFO Using network Socket
cn4070:3740707:3740872 [1] NCCL INFO NET/IB : No device found.
cn4070:3740707:3740872 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.101.13<0>
cn4070:3740707:3740872 [1] NCCL INFO Using non-device net plugin version 0
cn4070:3740707:3740872 [1] NCCL INFO Using network Socket
cn4070:3740708:3740873 [2] NCCL INFO NET/IB : No device found.
cn4070:3740708:3740873 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.101.13<0>
cn4070:3740708:3740873 [2] NCCL INFO Using non-device net plugin version 0
cn4070:3740708:3740873 [2] NCCL INFO Using network Socket
cn4070:3740706:3740870 [0] NCCL INFO comm 0x9e88440 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0x7b840662e7b44c97 - Init START
cn4070:3740707:3740872 [1] NCCL INFO comm 0xb4c9fc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 25000 commId 0x7b840662e7b44c97 - Init START
cn4070:3740708:3740873 [2] NCCL INFO comm 0x9e43e80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0x7b840662e7b44c97 - Init START
cn4070:3740709:3740871 [3] NCCL INFO comm 0xb9035c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0x7b840662e7b44c97 - Init START
cn4070:3740708:3740873 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,ffff0000
cn4070:3740708:3740873 [2] NCCL INFO NVLS multicast support is not available on dev 2
cn4070:3740709:3740871 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,ffff0000
cn4070:3740709:3740871 [3] NCCL INFO NVLS multicast support is not available on dev 3
cn4070:3740707:3740872 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
cn4070:3740707:3740872 [1] NCCL INFO NVLS multicast support is not available on dev 1
cn4070:3740706:3740870 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
cn4070:3740706:3740870 [0] NCCL INFO NVLS multicast support is not available on dev 0
cn4070:3740709:3740871 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
cn4070:3740709:3740871 [3] NCCL INFO P2P Chunksize set to 131072
cn4070:3740708:3740873 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
cn4070:3740708:3740873 [2] NCCL INFO P2P Chunksize set to 131072
cn4070:3740706:3740870 [0] NCCL INFO Channel 00/02 :    0   1   2   3
cn4070:3740706:3740870 [0] NCCL INFO Channel 01/02 :    0   1   2   3
cn4070:3740707:3740872 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
cn4070:3740706:3740870 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
cn4070:3740707:3740872 [1] NCCL INFO P2P Chunksize set to 131072
cn4070:3740706:3740870 [0] NCCL INFO P2P Chunksize set to 131072
cn4070:3740708:3740873 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
cn4070:3740709:3740871 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
cn4070:3740707:3740872 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
cn4070:3740708:3740873 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
cn4070:3740706:3740870 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
cn4070:3740709:3740871 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
cn4070:3740707:3740872 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
cn4070:3740706:3740870 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
cn4070:3740708:3740873 [2] NCCL INFO Connected all rings
cn4070:3740707:3740872 [1] NCCL INFO Connected all rings
cn4070:3740709:3740871 [3] NCCL INFO Connected all rings
cn4070:3740709:3740871 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
cn4070:3740706:3740870 [0] NCCL INFO Connected all rings
cn4070:3740708:3740873 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
cn4070:3740709:3740871 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
cn4070:3740708:3740873 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
cn4070:3740707:3740872 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
cn4070:3740707:3740872 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
cn4070:3740709:3740871 [3] NCCL INFO Connected all trees
cn4070:3740709:3740871 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn4070:3740709:3740871 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn4070:3740706:3740870 [0] NCCL INFO Connected all trees
cn4070:3740706:3740870 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn4070:3740706:3740870 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn4070:3740708:3740873 [2] NCCL INFO Connected all trees
cn4070:3740708:3740873 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn4070:3740708:3740873 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn4070:3740707:3740872 [1] NCCL INFO Connected all trees
cn4070:3740707:3740872 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn4070:3740707:3740872 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn4070:3740709:3740871 [3] NCCL INFO comm 0xb9035c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c1000 commId 0x7b840662e7b44c97 - Init COMPLETE
cn4070:3740707:3740872 [1] NCCL INFO comm 0xb4c9fc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 25000 commId 0x7b840662e7b44c97 - Init COMPLETE
cn4070:3740706:3740870 [0] NCCL INFO comm 0x9e88440 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1000 commId 0x7b840662e7b44c97 - Init COMPLETE
cn4070:3740708:3740873 [2] NCCL INFO comm 0x9e43e80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0x7b840662e7b44c97 - Init COMPLETE
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	logging_steps: 200 (from args) != 2500 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	logging_steps: 200 (from args) != 2500 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	logging_steps: 200 (from args) != 2500 (from trainer_state.json)
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	logging_steps: 200 (from args) != 2500 (from trainer_state.json)
  0%|          | 0/5000 [00:00<?, ?it/s]dataloader worker#0, ': Starting to iterate over 8/30 shards.
dataloader worker#0, ': Starting to iterate over 8/30 shards.
dataloader worker#1, ': Starting to iterate over 8/30 shards.
dataloader worker#1, ': Starting to iterate over 8/30 shards.
dataloader worker#2, ': Starting to iterate over 7/30 shards.
dataloader worker#0, ': Starting to iterate over 8/30 shards.
dataloader worker#2, ': Starting to iterate over 7/30 shards.
dataloader worker#3, ': Starting to iterate over 7/30 shards.
dataloader worker#1, ': Starting to iterate over 8/30 shards.
dataloader worker#3, ': Starting to iterate over 7/30 shards.
dataloader worker#2, ': Starting to iterate over 7/30 shards.
dataloader worker#3, ': Starting to iterate over 7/30 shards.
dataloader worker#0, ': Starting to iterate over 8/30 shards.
dataloader worker#1, ': Starting to iterate over 8/30 shards.
dataloader worker#2, ': Starting to iterate over 7/30 shards.
dataloader worker#3, ': Starting to iterate over 7/30 shards.
Batch of 10532913 bytes couldn't be parsed with block_size=327680. Retrying with block_size=655360.
Batch of 10532913 bytes couldn't be parsed with block_size=327680. Retrying with block_size=655360.
Batch of 10532913 bytes couldn't be parsed with block_size=327680. Retrying with block_size=655360.
Batch of 10532913 bytes couldn't be parsed with block_size=327680. Retrying with block_size=655360.
