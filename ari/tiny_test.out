[2024-06-03 12:09:51,801] torch.distributed.run: [WARNING] 
[2024-06-03 12:09:51,801] torch.distributed.run: [WARNING] *****************************************
[2024-06-03 12:09:51,801] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-03 12:09:51,801] torch.distributed.run: [WARNING] *****************************************
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/vast/home/ajherman/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
Config:
 GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

Number of model parameters: 124439808
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split train, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split train, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split train, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split train, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
No dataset info for wikitext available (HTTPError: 500 Server Error: Internal Server Error for url: https://datasets-server.huggingface.co/info?dataset=wikitext)
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split validation, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Finished loading datasets
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split validation, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Finished loading datasets
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split validation, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Finished loading datasets
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Constructing Dataset for split validation, from /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Finished loading datasets
Map:   0%|          | 0/36718 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Map:   0%|          | 0/36718 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Map:   0%|          | 0/36718 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Map:   0%|          | 0/36718 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-22bbe6333bd9aa51.arrow
Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-77bb4c13a6affc25.arrow
Map:   3%|â–Ž         | 1000/36718 [00:00<00:29, 1226.71 examples/s]Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f3b5adcd104d0d93.arrow
Map:   3%|â–Ž         | 1000/36718 [00:00<00:31, 1139.31 examples/s]Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d28b80b87af2f8f2.arrow
Map:   3%|â–Ž         | 1000/36718 [00:00<00:31, 1127.67 examples/s]Map:   3%|â–Ž         | 1000/36718 [00:00<00:31, 1131.21 examples/s]Map:   5%|â–Œ         | 2000/36718 [00:01<00:28, 1212.65 examples/s]Map:   5%|â–Œ         | 2000/36718 [00:01<00:29, 1163.52 examples/s]Map:   5%|â–Œ         | 2000/36718 [00:01<00:30, 1150.03 examples/s]Map:   5%|â–Œ         | 2000/36718 [00:01<00:30, 1154.34 examples/s]Map:   8%|â–Š         | 3000/36718 [00:02<00:25, 1323.11 examples/s]Map:   8%|â–Š         | 3000/36718 [00:02<00:26, 1254.65 examples/s]Map:   8%|â–Š         | 3000/36718 [00:02<00:27, 1236.92 examples/s]Map:   8%|â–Š         | 3000/36718 [00:02<00:27, 1244.37 examples/s]Map:  11%|â–ˆ         | 4000/36718 [00:02<00:23, 1382.08 examples/s]Map:  11%|â–ˆ         | 4000/36718 [00:03<00:25, 1297.28 examples/s]Map:  14%|â–ˆâ–Ž        | 5000/36718 [00:03<00:21, 1451.97 examples/s]Map:  11%|â–ˆ         | 4000/36718 [00:03<00:25, 1282.19 examples/s]Map:  11%|â–ˆ         | 4000/36718 [00:03<00:25, 1279.53 examples/s]Map:  14%|â–ˆâ–Ž        | 5000/36718 [00:03<00:23, 1360.19 examples/s]Map:  16%|â–ˆâ–‹        | 6000/36718 [00:04<00:20, 1506.39 examples/s]Map:  14%|â–ˆâ–Ž        | 5000/36718 [00:03<00:23, 1347.06 examples/s]Map:  14%|â–ˆâ–Ž        | 5000/36718 [00:03<00:23, 1337.56 examples/s]Map:  16%|â–ˆâ–‹        | 6000/36718 [00:04<00:21, 1405.09 examples/s]Map:  19%|â–ˆâ–‰        | 7000/36718 [00:04<00:20, 1465.24 examples/s]Map:  16%|â–ˆâ–‹        | 6000/36718 [00:04<00:22, 1390.19 examples/s]Map:  16%|â–ˆâ–‹        | 6000/36718 [00:04<00:22, 1380.57 examples/s]Map:  19%|â–ˆâ–‰        | 7000/36718 [00:05<00:21, 1366.22 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 8000/36718 [00:05<00:19, 1477.02 examples/s]Map:  19%|â–ˆâ–‰        | 7000/36718 [00:05<00:21, 1354.72 examples/s]Map:  19%|â–ˆâ–‰        | 7000/36718 [00:05<00:22, 1341.34 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 8000/36718 [00:06<00:20, 1373.56 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 9000/36718 [00:06<00:18, 1508.19 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 8000/36718 [00:06<00:21, 1362.00 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 8000/36718 [00:06<00:21, 1348.96 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 9000/36718 [00:06<00:19, 1402.67 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 10000/36718 [00:06<00:17, 1524.04 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 9000/36718 [00:06<00:19, 1392.66 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 9000/36718 [00:06<00:20, 1379.39 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 11000/36718 [00:07<00:16, 1531.99 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 10000/36718 [00:07<00:18, 1415.39 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 10000/36718 [00:07<00:18, 1413.19 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 10000/36718 [00:07<00:19, 1395.97 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36718 [00:08<00:15, 1553.64 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 11000/36718 [00:08<00:17, 1428.87 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 11000/36718 [00:08<00:18, 1420.03 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 11000/36718 [00:08<00:18, 1407.51 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:08<00:15, 1556.11 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36718 [00:08<00:17, 1448.21 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36718 [00:08<00:17, 1438.56 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36718 [00:08<00:17, 1429.03 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14000/36718 [00:09<00:14, 1592.11 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:09<00:16, 1443.81 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:09<00:16, 1434.88 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15000/36718 [00:10<00:13, 1606.08 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:09<00:16, 1420.39 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14000/36718 [00:10<00:15, 1475.52 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14000/36718 [00:10<00:15, 1468.35 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36718 [00:10<00:13, 1577.30 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14000/36718 [00:10<00:15, 1455.14 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15000/36718 [00:10<00:14, 1485.15 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15000/36718 [00:10<00:14, 1479.69 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/36718 [00:11<00:12, 1585.14 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15000/36718 [00:10<00:14, 1465.82 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36718 [00:11<00:14, 1458.36 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18000/36718 [00:11<00:11, 1603.44 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36718 [00:11<00:14, 1453.58 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36718 [00:11<00:14, 1440.85 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/36718 [00:12<00:13, 1469.14 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36718 [00:12<00:10, 1640.57 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/36718 [00:12<00:13, 1460.26 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/36718 [00:12<00:13, 1449.49 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18000/36718 [00:12<00:12, 1483.83 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36718 [00:13<00:10, 1642.80 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18000/36718 [00:12<00:12, 1472.45 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18000/36718 [00:12<00:12, 1465.46 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36718 [00:13<00:11, 1514.20 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36718 [00:13<00:09, 1626.86 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36718 [00:13<00:11, 1499.67 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36718 [00:13<00:11, 1497.94 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36718 [00:14<00:11, 1510.20 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22000/36718 [00:14<00:09, 1618.92 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36718 [00:14<00:11, 1502.35 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36718 [00:14<00:11, 1494.26 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 23000/36718 [00:14<00:08, 1635.36 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36718 [00:14<00:10, 1493.25 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36718 [00:14<00:10, 1485.12 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36718 [00:14<00:10, 1480.90 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36718 [00:15<00:07, 1670.56 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22000/36718 [00:15<00:09, 1487.15 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22000/36718 [00:15<00:09, 1478.44 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36718 [00:16<00:06, 1691.50 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22000/36718 [00:15<00:09, 1476.69 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 23000/36718 [00:16<00:09, 1502.64 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 23000/36718 [00:16<00:09, 1494.58 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26000/36718 [00:16<00:06, 1676.98 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 23000/36718 [00:16<00:09, 1487.06 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36718 [00:16<00:08, 1538.65 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36718 [00:16<00:08, 1530.69 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27000/36718 [00:17<00:05, 1640.61 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36718 [00:16<00:08, 1525.27 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36718 [00:17<00:07, 1559.46 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36718 [00:17<00:07, 1552.46 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36718 [00:17<00:05, 1613.86 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36718 [00:17<00:07, 1548.61 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26000/36718 [00:17<00:06, 1542.87 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26000/36718 [00:18<00:07, 1529.90 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 29000/36718 [00:18<00:04, 1647.14 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26000/36718 [00:18<00:06, 1533.42 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27000/36718 [00:18<00:06, 1509.37 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/36718 [00:19<00:04, 1650.29 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27000/36718 [00:18<00:06, 1499.78 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27000/36718 [00:18<00:06, 1497.76 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36718 [00:19<00:05, 1486.83 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31000/36718 [00:19<00:03, 1601.40 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36718 [00:19<00:05, 1478.99 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36718 [00:19<00:05, 1477.59 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 29000/36718 [00:20<00:05, 1521.15 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36718 [00:20<00:02, 1600.09 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 29000/36718 [00:20<00:05, 1513.29 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 29000/36718 [00:20<00:05, 1508.73 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/36718 [00:20<00:04, 1518.37 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33000/36718 [00:21<00:02, 1592.76 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/36718 [00:20<00:04, 1507.47 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/36718 [00:20<00:04, 1504.95 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31000/36718 [00:21<00:03, 1474.25 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36718 [00:21<00:01, 1591.11 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31000/36718 [00:21<00:03, 1466.09 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31000/36718 [00:21<00:03, 1460.28 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36718 [00:22<00:03, 1470.93 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 35000/36718 [00:22<00:01, 1624.14 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36718 [00:22<00:03, 1460.23 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36718 [00:22<00:03, 1459.15 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36718 [00:22<00:00, 1607.73 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33000/36718 [00:22<00:02, 1467.04 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33000/36718 [00:22<00:02, 1453.48 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:23<00:00, 1608.27 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33000/36718 [00:23<00:02, 1451.91 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36718 [00:23<00:01, 1463.57 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36718 [00:23<00:01, 1455.06 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36718 [00:23<00:01, 1460.26 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 35000/36718 [00:24<00:01, 1435.28 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 35000/36718 [00:24<00:01, 1500.41 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 35000/36718 [00:24<00:01, 1491.80 examples/s]Done writing 36718 examples in 199351621 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmp4d2hxpd6.
Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36718 [00:24<00:00, 1373.16 examples/s]Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:25<00:00, 1449.96 examples/s]
Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36718 [00:24<00:00, 1481.94 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36718 [00:25<00:00, 1473.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:25<00:00, 1374.20 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:25<00:00, 1483.63 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:25<00:00, 1484.98 examples/s]Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-68ac4c2444c697e2.arrow
Map:  27%|â–ˆâ–ˆâ–‹       | 1000/3760 [00:01<00:03, 800.97 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2000/3760 [00:02<00:01, 928.62 examples/s]Done writing 36718 examples in 199351621 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmp34844t2f.
Done writing 36718 examples in 199351621 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmp7qazmh5f.
Done writing 36718 examples in 199351621 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmp9un7s84p.
Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3000/3760 [00:03<00:00, 1001.20 examples/s]Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:31<00:00, 1183.90 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1033.68 examples/s]Done writing 3760 examples in 20440568 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmpojbmucv4.
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:31<00:00, 1184.41 examples/s]
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:31<00:00, 1184.41 examples/s]
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:04<00:00, 920.83 examples/s] 
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
cn3403:2012350:2012350 [0] NCCL INFO Bootstrap : Using eth0:192.168.100.156<0>
cn3403:2012350:2012350 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn3403:2012350:2012350 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.19.3+cuda12.3
cn3403:2012350:2012448 [0] NCCL INFO NET/IB : No device found.
cn3403:2012350:2012448 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.100.156<0>
cn3403:2012350:2012448 [0] NCCL INFO Using non-device net plugin version 0
cn3403:2012350:2012448 [0] NCCL INFO Using network Socket
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Map:   0%|          | 0/3760 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cb9231ab375e5d91.arrow
Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2937dc23548916f2.arrow
Map:  27%|â–ˆâ–ˆâ–‹       | 1000/3760 [00:00<00:02, 1197.45 examples/s]Caching processed dataset at /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-ca2314852d390b3f.arrow
Map:  27%|â–ˆâ–ˆâ–‹       | 1000/3760 [00:00<00:02, 1213.15 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 1000/3760 [00:00<00:02, 1216.92 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2000/3760 [00:01<00:01, 1189.49 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2000/3760 [00:01<00:01, 1197.38 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2000/3760 [00:01<00:01, 1202.71 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3000/3760 [00:02<00:00, 1224.07 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3000/3760 [00:02<00:00, 1225.58 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3000/3760 [00:02<00:00, 1230.64 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1253.65 examples/s]Done writing 3760 examples in 20440568 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmpdtyfu0ro.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1259.61 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1267.25 examples/s]Done writing 3760 examples in 20440568 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmpmpdb1z8s.
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1067.96 examples/s]
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Done writing 3760 examples in 20440568 bytes /vast/home/ajherman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/tmpkg81ghs8.
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1106.43 examples/s]
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Finished processing shard number None of 1.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:03<00:00, 1144.43 examples/s]
/vast/home/ajherman/transformers/src/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
cn3403:2012351:2012351 [1] NCCL INFO cudaDriverVersion 12040
cn3403:2012351:2012351 [1] NCCL INFO Bootstrap : Using eth0:192.168.100.156<0>
cn3403:2012351:2012351 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn3403:2012353:2012353 [3] NCCL INFO cudaDriverVersion 12040
cn3403:2012353:2012353 [3] NCCL INFO Bootstrap : Using eth0:192.168.100.156<0>
cn3403:2012353:2012353 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn3403:2012352:2012352 [2] NCCL INFO cudaDriverVersion 12040
cn3403:2012352:2012352 [2] NCCL INFO Bootstrap : Using eth0:192.168.100.156<0>
cn3403:2012352:2012352 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
cn3403:2012351:2012470 [1] NCCL INFO NET/IB : No device found.
cn3403:2012351:2012470 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.100.156<0>
cn3403:2012351:2012470 [1] NCCL INFO Using non-device net plugin version 0
cn3403:2012351:2012470 [1] NCCL INFO Using network Socket
cn3403:2012353:2012471 [3] NCCL INFO NET/IB : No device found.
cn3403:2012353:2012471 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.100.156<0>
cn3403:2012353:2012471 [3] NCCL INFO Using non-device net plugin version 0
cn3403:2012353:2012471 [3] NCCL INFO Using network Socket
cn3403:2012352:2012472 [2] NCCL INFO NET/IB : No device found.
cn3403:2012352:2012472 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.100.156<0>
cn3403:2012352:2012472 [2] NCCL INFO Using non-device net plugin version 0
cn3403:2012352:2012472 [2] NCCL INFO Using network Socket
cn3403:2012352:2012472 [2] NCCL INFO comm 0x9c92800 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xe4e96f4e83ff11a6 - Init START
cn3403:2012353:2012471 [3] NCCL INFO comm 0x9b1cb00 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 82000 commId 0xe4e96f4e83ff11a6 - Init START
cn3403:2012351:2012470 [1] NCCL INFO comm 0xb2e6480 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3000 commId 0xe4e96f4e83ff11a6 - Init START
cn3403:2012350:2012448 [0] NCCL INFO comm 0xb347b40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xe4e96f4e83ff11a6 - Init START
cn3403:2012352:2012472 [2] NCCL INFO Setting affinity for GPU 2 to fff0,00fff000
cn3403:2012352:2012472 [2] NCCL INFO NVLS multicast support is not available on dev 2
cn3403:2012351:2012470 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff
cn3403:2012351:2012470 [1] NCCL INFO NVLS multicast support is not available on dev 1
cn3403:2012350:2012448 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff
cn3403:2012350:2012448 [0] NCCL INFO NVLS multicast support is not available on dev 0
cn3403:2012353:2012471 [3] NCCL INFO Setting affinity for GPU 3 to fff0,00fff000
cn3403:2012353:2012471 [3] NCCL INFO NVLS multicast support is not available on dev 3
cn3403:2012353:2012471 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
cn3403:2012353:2012471 [3] NCCL INFO P2P Chunksize set to 131072
cn3403:2012352:2012472 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
cn3403:2012352:2012472 [2] NCCL INFO P2P Chunksize set to 131072
cn3403:2012350:2012448 [0] NCCL INFO Channel 00/02 :    0   1   2   3
cn3403:2012351:2012470 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
cn3403:2012350:2012448 [0] NCCL INFO Channel 01/02 :    0   1   2   3
cn3403:2012351:2012470 [1] NCCL INFO P2P Chunksize set to 131072
cn3403:2012350:2012448 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
cn3403:2012350:2012448 [0] NCCL INFO P2P Chunksize set to 131072
cn3403:2012353:2012471 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
cn3403:2012352:2012472 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
cn3403:2012353:2012471 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
cn3403:2012352:2012472 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
cn3403:2012350:2012448 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
cn3403:2012350:2012448 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
cn3403:2012351:2012470 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
cn3403:2012351:2012470 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
cn3403:2012350:2012448 [0] NCCL INFO Connected all rings
cn3403:2012351:2012470 [1] NCCL INFO Connected all rings
cn3403:2012352:2012472 [2] NCCL INFO Connected all rings
cn3403:2012353:2012471 [3] NCCL INFO Connected all rings
cn3403:2012353:2012471 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct
cn3403:2012353:2012471 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
cn3403:2012352:2012472 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
cn3403:2012352:2012472 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
cn3403:2012351:2012470 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
cn3403:2012351:2012470 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
cn3403:2012350:2012448 [0] NCCL INFO Connected all trees
cn3403:2012351:2012470 [1] NCCL INFO Connected all trees
cn3403:2012350:2012448 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn3403:2012350:2012448 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn3403:2012351:2012470 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn3403:2012351:2012470 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn3403:2012352:2012472 [2] NCCL INFO Connected all trees
cn3403:2012352:2012472 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn3403:2012353:2012471 [3] NCCL INFO Connected all trees
cn3403:2012352:2012472 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn3403:2012353:2012471 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
cn3403:2012353:2012471 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
cn3403:2012352:2012472 [2] NCCL INFO comm 0x9c92800 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 81000 commId 0xe4e96f4e83ff11a6 - Init COMPLETE
cn3403:2012353:2012471 [3] NCCL INFO comm 0x9b1cb00 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 82000 commId 0xe4e96f4e83ff11a6 - Init COMPLETE
cn3403:2012351:2012470 [1] NCCL INFO comm 0xb2e6480 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3000 commId 0xe4e96f4e83ff11a6 - Init COMPLETE
cn3403:2012350:2012448 [0] NCCL INFO comm 0xb347b40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xe4e96f4e83ff11a6 - Init COMPLETE
Exception occurred: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception occurred: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 3 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception occurred: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception occurred: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 1 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/160 [00:00<?, ?it/s]Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 219, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 219, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 825, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 813, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1312, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1126, in forward
    outputs = block(
              ^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 219, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 825, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 661, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 813, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1312, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 575, in forward
    hidden_states = self.act(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/ari/gpt2_train.py", line 219, in <module>
    trainer.train(resume_from_checkpoint=args.load_from_checkpoint) # More precise version would be to pass args.checkpoint_dir explicitly
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1126, in forward
    outputs = block(
              ^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
                                                                               ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 661, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 575, in forward
    hidden_states = self.act(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 3 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 825, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
                                                                               ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 813, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1312, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 825, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/accelerate/utils/operations.py", line 813, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 1 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1126, in forward
    outputs = block(
              ^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1312, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 661, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1126, in forward
    outputs = block(
              ^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 575, in forward
    hidden_states = self.act(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 661, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 575, in forward
    hidden_states = self.act(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
                                                                               ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
  File "/vast/home/ajherman/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/transformers/src/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
                                                                               ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 17.12 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 180.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
cn3403:2012350:2012476 [0] NCCL INFO [Service thread] Connection closed by localRank 0
cn3403:2012352:2012474 [2] NCCL INFO [Service thread] Connection closed by localRank 2
cn3403:2012353:2012473 [3] NCCL INFO [Service thread] Connection closed by localRank 3
cn3403:2012351:2012475 [1] NCCL INFO [Service thread] Connection closed by localRank 1
cn3403:2012350:2012350 [0] NCCL INFO comm 0xb347b40 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
cn3403:2012352:2012352 [2] NCCL INFO comm 0x9c92800 rank 2 nranks 4 cudaDev 2 busId 81000 - Abort COMPLETE
cn3403:2012353:2012353 [3] NCCL INFO comm 0x9b1cb00 rank 3 nranks 4 cudaDev 3 busId 82000 - Abort COMPLETE
cn3403:2012351:2012351 [1] NCCL INFO comm 0xb2e6480 rank 1 nranks 4 cudaDev 1 busId 3000 - Abort COMPLETE
